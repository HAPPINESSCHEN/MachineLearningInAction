{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.朴素贝叶斯\n",
    "## 调包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import feedparser\n",
    "from bayes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.收集数据：导入RSS源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ny['entries']的长度 = 60\n",
      "sy['entries']的长度 = 20\n"
     ]
    }
   ],
   "source": [
    "#使用国内能访问的RSS源\n",
    "ny = feedparser.parse(\"http://www.nasa.gov/rss/dyn/image_of_the_day.rss\")\n",
    "sy = feedparser.parse(\"http://rss.tom.com/happy/happy.xml\")\n",
    "print(\"ny['entries']的长度 = {}\".format(len(ny['entries'])))\n",
    "print(\"sy['entries']的长度 = {}\".format(len(sy['entries'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.RSS源分类器及高频词去除函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcMostFreq(vocabList,fullText):\n",
    "    \"\"\"\n",
    "    计算高频词\n",
    "    参数：\n",
    "        vocabList -- 词汇表\n",
    "        fullText -- 全文\n",
    "    返回：\n",
    "        最高频的30个词\n",
    "    \"\"\"\n",
    "    import operator\n",
    "    #新建频率统计字典\n",
    "    freqDict = {}\n",
    "    #遍历词汇表\n",
    "    for token in vocabList:\n",
    "        #统计每个词的次数\n",
    "        freqDict[token]=fullText.count(token)\n",
    "    #排序\n",
    "    sortedFreq = sorted(freqDict.items(), key=operator.itemgetter(1), reverse=True) \n",
    "    #返回前30个高频词汇\n",
    "    return sortedFreq[:30]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def localWords(feed1,feed0):\n",
    "    \"\"\"\n",
    "    RSS源分类测试函数\n",
    "    参数：\n",
    "        feed1 -- 输入文本1\n",
    "        feed2 -- 输入文本2\n",
    "    返回：\n",
    "        vocabList -- 词汇表\n",
    "        p0V -- 类型0的概率\n",
    "        p1V -- 类型1的概率\n",
    "    \"\"\"\n",
    "    import feedparser\n",
    "    #新建列表\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    #取两者之间的最小的长度\n",
    "    minLen = min(len(feed1['entries']),len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        #取输入1的文本\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        #添加到列表\n",
    "        docList.append(wordList)\n",
    "        #延长到列表\n",
    "        fullText.extend(wordList)\n",
    "        #添加类别列表\n",
    "        classList.append(1)\n",
    "        #输入0的文本\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    #创建词汇表\n",
    "    vocabList = createVocabList(docList)\n",
    "    #去掉最高频率的30个词汇\n",
    "    top30Words = calcMostFreq(vocabList,fullText)\n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList: vocabList.remove(pairW[0])\n",
    "    #训练集和测试集的索引\n",
    "    trainingSet = list(range(2*minLen)); testSet=[]\n",
    "    #随机产生20个索引，从训练集移动到测试集\n",
    "    for i in range(20):\n",
    "        randIndex = np.int(np.random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    #训练集数据和标签\n",
    "    trainMat=[]; trainClasses = []\n",
    "    #添加数据\n",
    "    for docIndex in trainingSet:\n",
    "        #把词袋数据添加到训练集数据矩阵中\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        #添加训练标签\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    #训练朴素贝叶斯分类器的得到三个概率\n",
    "    p0V,p1V,pSpam = trainNB0(np.array(trainMat),np.array(trainClasses))\n",
    "    #初始化错误计数器\n",
    "    errorCount = 0\n",
    "    #测试\n",
    "    for docIndex in testSet:\n",
    "        #转化为词袋\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        #统计错误\n",
    "        if classifyNB(np.array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    #打印准确率\n",
    "    print(\"准确率为: {}\".format(1 - float(errorCount)/len(testSet)))\n",
    "    return vocabList,p0V,p1V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率为: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "vocabList, pNY, pSY = localWords(ny,sy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.得到最有特征的词汇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopWords(ny,sf):\n",
    "    \"\"\"\n",
    "    得到最有特征的词汇\n",
    "    参数：\n",
    "        ny -- 输入文本1\n",
    "        sf -- 输入文本2\n",
    "    返回：\n",
    "        无\n",
    "    \"\"\"\n",
    "    import operator\n",
    "    #对两个文本进行朴素贝叶斯分类\n",
    "    vocabList,p0V,p1V=localWords(ny,sf)\n",
    "    #新建两个列表来存储最有特征的词汇\n",
    "    topNY=[]; topSF=[]\n",
    "    #遍历两个词汇表的所有概率，选取较大的添加到特征词汇表中\n",
    "    for i in range(p0V.shape[1]):\n",
    "        if p0V[0][i] > -6.0 : topSF.append((vocabList[i],p0V[0][i]))\n",
    "        if p1V[0][i] > -6.0 : topNY.append((vocabList[i],p1V[0][i]))\n",
    "    #对SF特征词汇表排序\n",
    "    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)\n",
    "    #打印SF\n",
    "    print(\"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\")\n",
    "    for item in sortedSF:\n",
    "        print(item[0])\n",
    "    #对NY进行类似操作\n",
    "    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)\n",
    "    print(\"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\")\n",
    "    for item in sortedNY:\n",
    "        print(item[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率为: 0.5\n",
      "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\n",
      "northern\n",
      "parker\n",
      "earth\n",
      "families\n",
      "schirra\n",
      "nick\n",
      "instruments\n",
      "jets\n",
      "home\n",
      "again\n",
      "members\n",
      "transits\n",
      "joe\n",
      "fly\n",
      "juno\n",
      "serena\n",
      "veggie\n",
      "sun\n",
      "nanoracks\n",
      "aeronautics\n",
      "feustel\n",
      "these\n",
      "air\n",
      "designed\n",
      "procedures\n",
      "installs\n",
      "fire\n",
      "than\n",
      "preparation\n",
      "been\n",
      "hasn\n",
      "gulf\n",
      "acronym\n",
      "oval\n",
      "views\n",
      "arnold\n",
      "lopez\n",
      "donn\n",
      "works\n",
      "better\n",
      "force\n",
      "acaba\n",
      "revolution\n",
      "southern\n",
      "california\n",
      "berenices\n",
      "jupiter\n",
      "spacewalk\n",
      "reduce\n",
      "show\n",
      "examines\n",
      "delta\n",
      "1930\n",
      "ovchinin\n",
      "out\n",
      "only\n",
      "cosmonaut\n",
      "taken\n",
      "crews\n",
      "missions\n",
      "commander\n",
      "satellite\n",
      "landing\n",
      "hardware\n",
      "well\n",
      "hour\n",
      "launches\n",
      "over\n",
      "gerst\n",
      "launch\n",
      "embrace\n",
      "can\n",
      "alexander\n",
      "airport\n",
      "october\n",
      "that\n",
      "observations\n",
      "debris\n",
      "gear\n",
      "retriever\n",
      "data\n",
      "sept\n",
      "mission\n",
      "alegria\n",
      "least\n",
      "rolled\n",
      "just\n",
      "approach\n",
      "color\n",
      "plant\n",
      "together\n",
      "alexey\n",
      "early\n",
      "gemini\n",
      "reveals\n",
      "composite\n",
      "ricky\n",
      "sunday\n",
      "barge\n",
      "per\n",
      "four\n",
      "minute\n",
      "junk\n",
      "where\n",
      "pad\n",
      "serving\n",
      "programs\n",
      "shows\n",
      "three\n",
      "growth\n",
      "presented\n",
      "remove\n",
      "science\n",
      "see\n",
      "lies\n",
      "shuttle\n",
      "eye\n",
      "captured\n",
      "former\n",
      "her\n",
      "born\n",
      "constellation\n",
      "funduscope\n",
      "aboard\n",
      "observatory\n",
      "krayniy\n",
      "thousand\n",
      "continues\n",
      "opportunity\n",
      "tuesday\n",
      "probe\n",
      "coma\n",
      "suites\n",
      "features\n",
      "chancellor\n",
      "student\n",
      "mexico\n",
      "impressive\n",
      "cluster\n",
      "risks\n",
      "left\n",
      "belt\n",
      "deployed\n",
      "facility\n",
      "working\n",
      "our\n",
      "five\n",
      "still\n",
      "first\n",
      "hague\n",
      "construction\n",
      "dynamics\n",
      "lands\n",
      "enhanced\n",
      "different\n",
      "feb\n",
      "roughly\n",
      "right\n",
      "wavelength\n",
      "silhouette\n",
      "artemyev\n",
      "egress\n",
      "demonstrate\n",
      "oleg\n",
      "practiced\n",
      "but\n",
      "its\n",
      "photo\n",
      "young\n",
      "train\n",
      "planet\n",
      "agency\n",
      "commercial\n",
      "water\n",
      "rocket\n",
      "brown\n",
      "gravity\n",
      "investigation\n",
      "rover\n",
      "light\n",
      "drew\n",
      "experiment\n",
      "solar\n",
      "miles\n",
      "michael\n",
      "2007\n",
      "auñón\n",
      "known\n",
      "has\n",
      "their\n",
      "orbital\n",
      "walter\n",
      "south\n",
      "aug\n",
      "into\n",
      "18th\n",
      "structure\n",
      "bound\n",
      "john\n",
      "longest\n",
      "participates\n",
      "long\n",
      "women\n",
      "returned\n",
      "wavelengths\n",
      "teacher\n",
      "vandenberg\n",
      "roscosmos\n",
      "heard\n",
      "ten\n",
      "saturday\n",
      "second\n",
      "most\n",
      "icesat\n",
      "soyuz\n",
      "small\n",
      "galaxies\n",
      "instrument\n",
      "using\n",
      "always\n",
      "botany\n",
      "after\n",
      "eisele\n",
      "view\n",
      "equatorial\n",
      "because\n",
      "part\n",
      "spacecraft\n",
      "cunningham\n",
      "base\n",
      "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "getTopWords(ny, sy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
